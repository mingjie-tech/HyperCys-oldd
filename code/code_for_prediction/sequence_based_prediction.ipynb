{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74a6513a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data scaled\n",
      "Fitting 6 classifiers...\n",
      "Fitting classifier1: pipeline (1/6)\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('svc',\n",
      "                 SVC(C=100, gamma=0.001, kernel='sigmoid', probability=True,\n",
      "                     random_state=42))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    4.7s remaining:    1.9s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    4.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting classifier2: pipeline (2/6)\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('kneighborsclassifier', KNeighborsClassifier(n_neighbors=9))])\n",
      "Fitting classifier3: pipeline (3/6)\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('logisticregression', LogisticRegression(C=0.1))])\n",
      "Fitting classifier4: pipeline (4/6)\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('lgbmclassifier',\n",
      "                 LGBMClassifier(colsample_bytree=0.3, max_depth=15,\n",
      "                                num_leaves=50))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    3.0s remaining:    1.2s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    3.0s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting classifier5: pipeline (5/6)\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('mlpclassifier',\n",
      "                 MLPClassifier(alpha=10, hidden_layer_sizes=(50, 50, 50),\n",
      "                               learning_rate='adaptive'))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    1.4s remaining:    0.5s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    1.8s finished\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting classifier6: pipeline (6/6)\n",
      "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('randomforestclassifier',\n",
      "                 RandomForestClassifier(max_depth=3000, min_samples_split=4,\n",
      "                                        n_estimators=80))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   7 out of  10 | elapsed:    0.2s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  10 | elapsed:    0.3s finished\n",
      "C:\\Users\\Administrator.DESKTOP-8GVJ8BS\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model file created\n",
      "saving model file in disk\n",
      "model file saved\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pk\n",
    "import math\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import lightgbm as lgb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, accuracy_score, precision_score, confusion_matrix, recall_score, f1_score, auc, matthews_corrcoef\n",
    "from imblearn.pipeline import make_pipeline\n",
    "from mlxtend.classifier import StackingCVClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "\n",
    "# read the training data file\n",
    "covalent = pd.read_excel('D:/JCIM-write/HyperCys/data/train_dataset.xlsx')\n",
    "\n",
    "covalent=covalent.loc[:,['label','helix','strand','coil','accessible_surface_area','PSSM_1','PSSM_2','PSSM_3','PSSM_4','PSSM_5','PSSM_6',\n",
    "                         'PSSM_7','PSSM_8','PSSM_9','PSSM_10','PSSM_11','PSSM_12','PSSM_13','PSSM_14','PSSM_15','PSSM_16','PSSM_17','PSSM_18',\n",
    "                         'PSSM_19','PSSM_20','monogram','bigram_1','bigram_2','bigram_3','bigram_4','bigram_5','bigram_6','bigram_7',\n",
    "                         'bigram_8','bigram_9','bigram_10','bigram_11','bigram_12','bigram_13','bigram_14','bigram_15','bigram_16',\n",
    "                         'bigram_17','bigram_18','bigram_19','bigram_20','PSEE']]#Sequence-based\n",
    "X=covalent.loc[:, covalent.columns != 'label']\n",
    "y=covalent['label']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "print('data scaled')\n",
    "# Six different classifiers\n",
    "classifier1 = SVC(C=100,  gamma= 0.001, kernel=\"sigmoid\",probability=True,random_state=42)\n",
    "\n",
    "classifier2 = KNeighborsClassifier(9)\n",
    "\n",
    "classifier3 = LogisticRegression(C=0.1)\n",
    "\n",
    "classifier4 = lgb.LGBMClassifier(colsample_bytree=0.3,max_depth=15,n_estimators=100, num_leaves=50)\n",
    "\n",
    "classifier5=MLPClassifier(alpha= 10, hidden_layer_sizes=(50, 50, 50), learning_rate='adaptive', solver='adam')\n",
    "\n",
    "classifier6= RandomForestClassifier(criterion=\"gini\", max_depth= 3000, min_samples_split= 4, n_estimators= 80) # Define classifier\n",
    "# stacked classifier\n",
    "clf = StackingCVClassifier(classifiers = [make_pipeline(scaler, classifier1), make_pipeline(scaler, classifier2), make_pipeline(scaler,  classifier3), make_pipeline(scaler,  classifier4), make_pipeline(scaler,  classifier5), make_pipeline(scaler,  classifier6)],\n",
    "\t\t\t\t\t\t\tshuffle = False,\n",
    "\t\t\t\t\t\t\tuse_probas = True,\n",
    "\t\t\t\t\t\t\tcv = 10,\n",
    "\t\t\t\t\t\t   verbose=2,\n",
    "\t\t\t\t\t\t   n_jobs=-1,\n",
    "\t\t\t\t\t\t\tstore_train_meta_features = True,\n",
    "\t\t\t\t\t\t\tuse_features_in_secondary =True,\n",
    "\t\t\t\t\t\t\tmeta_classifier =make_pipeline(scaler, LogisticRegression(C=0.1))\n",
    ")\n",
    "\n",
    "\n",
    "clf.fit(X,y)\n",
    "print('model file created')\n",
    "print('saving model file in disk')\n",
    "# save the model to disk\n",
    "modelfile = 'D:/JCIM-write/HyperCys/model/sequence_based.model'\n",
    "joblib.dump(clf, modelfile)\n",
    "print('model file saved')\n",
    "\n",
    "\n",
    "\n",
    "X_test = pd.read_excel('D:/JCIM-write/HyperCys/data/test_dataset.xlsx')\n",
    "X_test=X_test.loc[:,['helix','strand','coil','accessible_surface_area','PSSM_1','PSSM_2','PSSM_3','PSSM_4','PSSM_5','PSSM_6',\n",
    "                         'PSSM_7','PSSM_8','PSSM_9','PSSM_10','PSSM_11','PSSM_12','PSSM_13','PSSM_14','PSSM_15','PSSM_16','PSSM_17','PSSM_18',\n",
    "                         'PSSM_19','PSSM_20','monogram','bigram_1','bigram_2','bigram_3','bigram_4','bigram_5','bigram_6','bigram_7',\n",
    "                         'bigram_8','bigram_9','bigram_10','bigram_11','bigram_12','bigram_13','bigram_14','bigram_15','bigram_16',\n",
    "                         'bigram_17','bigram_18','bigram_19','bigram_20','PSEE']]\n",
    "\n",
    "\n",
    "X_test = scaler.transform(X_test) # the scaler instance is used on test data to transform it the same way it did on the training set\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred_prob = clf.predict_proba(X_test)\n",
    "y_pred = np.column_stack([y_pred, y_pred_prob])\n",
    "\n",
    "#save the prediction output of test data\n",
    "output_file = 'D:/JCIM-write/HyperCys/model/sequence_based_prediction.predict'\n",
    "out_file = open(output_file, 'wb')\n",
    "np.savetxt(fname=output_file, X=y_pred, fmt='%d %0.4f %0.4f', header='predClass, probNonCov, probCov', comments='')\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dcf4f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
